

######################################################################
# STARTING FULL TRAINING FOR reflected
# RUN NAME: reflected_2025-11-15_17-41-09
######################################################################

######################################################################
# FULL BACKWARD TRAINING: reflected
######################################################################


======================================================================
TRAINING TIMESTEP n=50/50 (t=1.000)
======================================================================

============================================================
Training timestep n=50 (TERMINAL STEP)
Time: t=1.000, Future models: 0
============================================================
Iter    0: loss=2.745090e-01 | grad=2.7719e+00 | lr=1.00e-02
Iter    1: loss=6.062028e-01 | grad=3.2367e+00 | lr=1.00e-02
Iter    2: loss=3.992207e-01 | grad=2.9308e+00 | lr=1.00e-02
Iter    3: loss=6.443357e-02 | grad=6.8863e-01 | lr=1.00e-02
Iter    4: loss=2.993246e-01 | grad=2.4194e+00 | lr=1.00e-02
Iter    5: loss=3.151944e-01 | grad=2.2753e+00 | lr=1.00e-02
Iter    6: loss=2.097524e-01 | grad=2.1326e+00 | lr=1.00e-02
Iter    7: loss=6.424300e-02 | grad=9.4408e-01 | lr=1.00e-02
Iter    8: loss=1.062065e-01 | grad=1.6124e+00 | lr=1.00e-02
Iter    9: loss=1.413239e-01 | grad=1.9174e+00 | lr=1.00e-02
Iter  100: loss=2.349301e-02 | grad=8.9455e-01 | lr=1.00e-02
Iter  200: loss=2.035747e-02 | grad=9.0006e-01 | lr=1.00e-02
Iter  300: loss=1.297506e-02 | grad=1.0118e+00 | lr=1.00e-02
Iter  400: loss=9.479638e-03 | grad=9.5107e-01 | lr=5.00e-03
Iter  500: loss=5.747223e-03 | grad=4.0084e-01 | lr=5.00e-03
Iter  600: loss=5.463527e-03 | grad=9.6920e-02 | lr=5.00e-03
Iter  700: loss=5.392164e-03 | grad=1.3415e-01 | lr=2.50e-03
Iter  800: loss=5.365550e-03 | grad=2.0277e-02 | lr=2.50e-03
Iter  900: loss=5.340665e-03 | grad=6.0111e-02 | lr=6.25e-04
Iter 1000: loss=5.293496e-03 | grad=8.5069e-02 | lr=3.13e-04
Iter 1100: loss=5.439533e-03 | grad=1.3780e-02 | lr=1.56e-04
Iter 1200: loss=5.307171e-03 | grad=3.7340e-02 | lr=3.91e-05
Iter 1300: loss=5.348956e-03 | grad=3.4801e-02 | lr=1.95e-05
Iter 1400: loss=5.468678e-03 | grad=2.5291e-02 | lr=4.88e-06
✓ Saved loss history for timestep 50 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_50.json

 Final loss: 5.155136e-03
Iterations used: 1500

======================================================================
TRAINING TIMESTEP n=49/50 (t=0.980)
======================================================================

============================================================
Training timestep n=49 (BACKWARD STEP)
Time: t=0.980, Future models: 1
============================================================
Iter    0: loss=4.989152e-02 | grad=5.3949e-01 | lr=9.95e-03
Iter    1: loss=2.657098e-01 | grad=7.4799e-01 | lr=9.95e-03
Iter    2: loss=1.609349e-01 | grad=8.0829e-01 | lr=9.95e-03
Iter    3: loss=8.253640e-02 | grad=2.8017e-01 | lr=9.95e-03
Iter    4: loss=1.022144e-01 | grad=7.5612e-01 | lr=9.95e-03
Iter    5: loss=5.007030e-02 | grad=4.9597e-01 | lr=9.95e-03
Iter    6: loss=8.520348e-02 | grad=1.4024e+00 | lr=9.95e-03
Iter    7: loss=7.555641e-02 | grad=4.9602e-01 | lr=9.95e-03
Iter    8: loss=6.091775e-02 | grad=8.4504e-01 | lr=9.95e-03
Iter    9: loss=4.479151e-02 | grad=1.2298e+00 | lr=9.95e-03
Iter  100: loss=1.405533e-02 | grad=1.0359e+00 | lr=9.95e-03
Iter  200: loss=6.371958e-03 | grad=3.7013e-01 | lr=4.98e-03
Iter  300: loss=6.046847e-03 | grad=4.1971e-01 | lr=4.98e-03
Iter  400: loss=5.722934e-03 | grad=8.5521e-02 | lr=2.49e-03
✓ Saved loss history for timestep 49 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_49.json

 Final loss: 5.725830e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=48/50 (t=0.960)
======================================================================

============================================================
Training timestep n=48 (BACKWARD STEP)
Time: t=0.960, Future models: 2
============================================================
Iter    0: loss=9.928063e-03 | grad=1.3092e+00 | lr=9.90e-03
Iter    1: loss=1.228988e-01 | grad=9.6309e-01 | lr=9.90e-03
Iter    2: loss=8.726038e-02 | grad=4.1083e-01 | lr=9.90e-03
Iter    3: loss=8.132083e-02 | grad=5.5195e-01 | lr=9.90e-03
Iter    4: loss=7.380141e-02 | grad=6.9218e-01 | lr=9.90e-03
Iter    5: loss=6.156036e-02 | grad=4.0160e-01 | lr=9.90e-03
Iter    6: loss=5.750434e-02 | grad=3.4779e-01 | lr=9.90e-03
Iter    7: loss=5.190342e-02 | grad=3.8893e-01 | lr=9.90e-03
Iter    8: loss=5.075465e-02 | grad=8.7583e-01 | lr=9.90e-03
Iter    9: loss=4.787170e-02 | grad=7.0975e-01 | lr=9.90e-03
Iter  100: loss=9.458317e-03 | grad=6.7388e-01 | lr=4.95e-03
Iter  200: loss=7.173641e-03 | grad=7.1935e-01 | lr=4.95e-03
Iter  300: loss=6.229494e-03 | grad=4.2185e-02 | lr=2.48e-03
Iter  400: loss=5.643888e-03 | grad=9.2113e-02 | lr=1.24e-03
✓ Saved loss history for timestep 48 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_48.json

 Final loss: 5.269581e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=47/50 (t=0.940)
======================================================================

============================================================
Training timestep n=47 (BACKWARD STEP)
Time: t=0.940, Future models: 3
============================================================
Iter    0: loss=9.050578e-03 | grad=1.3195e+00 | lr=9.85e-03
Iter    1: loss=1.272179e-01 | grad=1.1221e+00 | lr=9.85e-03
Iter    2: loss=1.027191e-01 | grad=4.5209e-01 | lr=9.85e-03
Iter    3: loss=1.106688e-01 | grad=1.0243e+00 | lr=9.85e-03
Iter    4: loss=7.245661e-02 | grad=2.7217e-01 | lr=9.85e-03
Iter    5: loss=1.053444e-01 | grad=2.0354e+00 | lr=9.85e-03
Iter    6: loss=6.943096e-02 | grad=7.4617e-01 | lr=9.85e-03
Iter    7: loss=6.887026e-02 | grad=7.5995e-01 | lr=9.85e-03
Iter    8: loss=8.003694e-02 | grad=9.7690e-01 | lr=9.85e-03
Iter    9: loss=6.587766e-02 | grad=7.1850e-01 | lr=9.85e-03
Iter  100: loss=1.499892e-02 | grad=2.5614e-01 | lr=4.93e-03
Iter  200: loss=1.227959e-02 | grad=1.7369e-01 | lr=1.23e-03
Iter  300: loss=1.005338e-02 | grad=3.1506e-02 | lr=3.08e-04
Iter  400: loss=9.781851e-03 | grad=6.6950e-02 | lr=7.70e-05
✓ Saved loss history for timestep 47 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_47.json

 Final loss: 1.048095e-02
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=46/50 (t=0.920)
======================================================================

============================================================
Training timestep n=46 (BACKWARD STEP)
Time: t=0.920, Future models: 4
============================================================
Iter    0: loss=1.345676e-02 | grad=1.4871e+00 | lr=9.80e-03
Iter    1: loss=2.899430e-01 | grad=2.4764e+00 | lr=9.80e-03
Iter    2: loss=2.206945e-01 | grad=1.6239e+00 | lr=9.80e-03
Iter    3: loss=6.419958e-02 | grad=4.4137e-01 | lr=9.80e-03
Iter    4: loss=1.495930e-01 | grad=2.3484e+00 | lr=9.80e-03
Iter    5: loss=1.117164e-01 | grad=2.0601e+00 | lr=9.80e-03
Iter    6: loss=6.504801e-02 | grad=3.1337e-01 | lr=9.80e-03
Iter    7: loss=1.002401e-01 | grad=1.9189e+00 | lr=9.80e-03
Iter    8: loss=9.997890e-02 | grad=1.7635e+00 | lr=9.80e-03
Iter    9: loss=6.578991e-02 | grad=8.7821e-01 | lr=9.80e-03
Iter  100: loss=7.910857e-03 | grad=5.8609e-01 | lr=9.80e-03
Iter  200: loss=1.374586e-02 | grad=1.2791e+00 | lr=9.80e-03
Iter  300: loss=3.845464e-03 | grad=8.5750e-01 | lr=4.90e-03
Iter  400: loss=3.508336e-03 | grad=6.5447e-01 | lr=4.90e-03
✓ Saved loss history for timestep 46 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_46.json

 Final loss: 2.583630e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=45/50 (t=0.900)
======================================================================

============================================================
Training timestep n=45 (BACKWARD STEP)
Time: t=0.900, Future models: 5
============================================================
Iter    0: loss=8.038254e-03 | grad=1.1521e+00 | lr=9.75e-03
Iter    1: loss=8.381519e-02 | grad=3.4873e-01 | lr=9.75e-03
Iter    2: loss=8.387902e-02 | grad=5.4974e-01 | lr=9.75e-03
Iter    3: loss=8.771337e-02 | grad=7.7704e-01 | lr=9.75e-03
Iter    4: loss=5.299617e-02 | grad=1.1977e+00 | lr=9.75e-03
Iter    5: loss=1.170657e-01 | grad=2.3983e+00 | lr=9.75e-03
Iter    6: loss=4.839004e-02 | grad=5.0343e-01 | lr=9.75e-03
Iter    7: loss=1.065683e-01 | grad=2.1382e+00 | lr=9.75e-03
Iter    8: loss=7.720254e-02 | grad=1.6192e+00 | lr=9.75e-03
Iter    9: loss=4.726666e-02 | grad=8.4634e-01 | lr=9.75e-03
Iter  100: loss=1.374911e-02 | grad=1.5456e+00 | lr=4.88e-03
Iter  200: loss=5.400981e-03 | grad=2.9798e-01 | lr=2.44e-03
Iter  300: loss=5.122840e-03 | grad=2.3781e-01 | lr=2.44e-03
Iter  400: loss=4.629134e-03 | grad=2.6312e-01 | lr=1.22e-03
✓ Saved loss history for timestep 45 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_45.json

 Final loss: 4.393113e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=44/50 (t=0.880)
======================================================================

============================================================
Training timestep n=44 (BACKWARD STEP)
Time: t=0.880, Future models: 6
============================================================
Iter    0: loss=9.528951e-03 | grad=1.6246e+00 | lr=9.70e-03
Iter    1: loss=2.496855e-01 | grad=2.8876e+00 | lr=9.70e-03
Iter    2: loss=9.354678e-02 | grad=1.2709e+00 | lr=9.70e-03
Iter    3: loss=1.939835e-01 | grad=3.4603e+00 | lr=9.70e-03
Iter    4: loss=1.501079e-01 | grad=3.2919e+00 | lr=9.70e-03
Iter    5: loss=8.101375e-02 | grad=5.9322e-01 | lr=9.70e-03
Iter    6: loss=1.242591e-01 | grad=3.4660e+00 | lr=9.70e-03
Iter    7: loss=1.190159e-01 | grad=2.8557e+00 | lr=9.70e-03
Iter    8: loss=6.815207e-02 | grad=1.0207e+00 | lr=9.70e-03
Iter    9: loss=7.509378e-02 | grad=1.7733e+00 | lr=9.70e-03
Iter  100: loss=1.321699e-02 | grad=9.4390e-02 | lr=4.85e-03
Iter  200: loss=9.763705e-03 | grad=7.8714e-02 | lr=1.21e-03
Iter  300: loss=7.474170e-03 | grad=5.4911e-01 | lr=1.21e-03
Iter  400: loss=6.029652e-03 | grad=1.6886e-01 | lr=1.21e-03
✓ Saved loss history for timestep 44 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_44.json

 Final loss: 4.827127e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=43/50 (t=0.860)
======================================================================

============================================================
Training timestep n=43 (BACKWARD STEP)
Time: t=0.860, Future models: 7
============================================================
Iter    0: loss=9.952577e-03 | grad=2.0833e+00 | lr=9.66e-03
Iter    1: loss=2.703291e-01 | grad=2.7899e+00 | lr=9.66e-03
Iter    2: loss=1.061091e-01 | grad=2.9672e+00 | lr=9.66e-03
Iter    3: loss=1.771786e-01 | grad=3.7282e+00 | lr=9.66e-03
Iter    4: loss=1.617784e-01 | grad=3.8868e+00 | lr=9.66e-03
Iter    5: loss=7.043098e-02 | grad=1.8720e+00 | lr=9.66e-03
Iter    6: loss=1.249750e-01 | grad=3.6884e+00 | lr=9.66e-03
Iter    7: loss=1.545405e-01 | grad=3.5807e+00 | lr=9.66e-03
Iter    8: loss=1.020969e-01 | grad=3.5312e+00 | lr=9.66e-03
Iter    9: loss=6.284066e-02 | grad=6.0321e-01 | lr=9.66e-03
Iter  100: loss=1.267158e-02 | grad=1.5725e+00 | lr=4.83e-03
Iter  200: loss=7.572931e-03 | grad=1.3314e-01 | lr=2.41e-03
Iter  300: loss=5.193147e-03 | grad=1.2023e-01 | lr=2.41e-03
Iter  400: loss=4.620988e-03 | grad=5.7526e-01 | lr=2.41e-03
✓ Saved loss history for timestep 43 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_43.json

 Final loss: 3.850391e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=42/50 (t=0.840)
======================================================================

============================================================
Training timestep n=42 (BACKWARD STEP)
Time: t=0.840, Future models: 8
============================================================
Iter    0: loss=9.718833e-03 | grad=1.8049e+00 | lr=9.61e-03
Iter    1: loss=1.933283e-01 | grad=3.2377e+00 | lr=9.61e-03
Iter    2: loss=9.929258e-02 | grad=1.2992e+00 | lr=9.61e-03
Iter    3: loss=1.200370e-01 | grad=2.8096e+00 | lr=9.61e-03
Iter    4: loss=6.923503e-02 | grad=6.8574e-01 | lr=9.61e-03
Iter    5: loss=1.023172e-01 | grad=3.7118e+00 | lr=9.61e-03
Iter    6: loss=8.062695e-02 | grad=2.1051e+00 | lr=9.61e-03
Iter    7: loss=6.152687e-02 | grad=1.7743e+00 | lr=9.61e-03
Iter    8: loss=8.077657e-02 | grad=2.4433e+00 | lr=9.61e-03
Iter    9: loss=6.216513e-02 | grad=1.6209e+00 | lr=9.61e-03
Iter  100: loss=3.000327e-02 | grad=2.3090e+00 | lr=4.80e-03
Iter  200: loss=1.145681e-02 | grad=4.5207e-01 | lr=1.20e-03
Iter  300: loss=1.061721e-02 | grad=6.9590e-02 | lr=3.00e-04
Iter  400: loss=1.023030e-02 | grad=5.3673e-02 | lr=1.50e-04
✓ Saved loss history for timestep 42 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_42.json

 Final loss: 9.566566e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=41/50 (t=0.820)
======================================================================

============================================================
Training timestep n=41 (BACKWARD STEP)
Time: t=0.820, Future models: 9
============================================================
Iter    0: loss=1.216268e-02 | grad=2.1335e+00 | lr=9.56e-03
Iter    1: loss=3.753538e-01 | grad=4.3695e+00 | lr=9.56e-03
Iter    2: loss=2.722373e-01 | grad=3.2683e+00 | lr=9.56e-03
Iter    3: loss=6.359139e-02 | grad=7.9595e-01 | lr=9.56e-03
Iter    4: loss=1.761237e-01 | grad=4.6669e+00 | lr=9.56e-03
Iter    5: loss=1.394043e-01 | grad=4.0141e+00 | lr=9.56e-03
Iter    6: loss=5.980489e-02 | grad=7.8715e-01 | lr=9.56e-03
Iter    7: loss=1.147772e-01 | grad=4.6309e+00 | lr=9.56e-03
Iter    8: loss=1.183434e-01 | grad=3.9220e+00 | lr=9.56e-03
Iter    9: loss=6.787804e-02 | grad=2.4275e+00 | lr=9.56e-03
Iter  100: loss=1.198236e-02 | grad=1.2309e+00 | lr=4.78e-03
Iter  200: loss=7.534219e-03 | grad=1.0989e+00 | lr=4.78e-03
Iter  300: loss=5.470922e-03 | grad=1.1637e+00 | lr=4.78e-03
Iter  400: loss=4.825052e-03 | grad=1.9479e+00 | lr=2.39e-03
✓ Saved loss history for timestep 41 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_41.json

 Final loss: 3.666303e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=40/50 (t=0.800)
======================================================================

============================================================
Training timestep n=40 (BACKWARD STEP)
Time: t=0.800, Future models: 10
============================================================
Iter    0: loss=6.054011e-03 | grad=1.8611e+00 | lr=9.51e-03
Iter    1: loss=1.915551e-01 | grad=3.7886e+00 | lr=9.51e-03
Iter    2: loss=8.371342e-02 | grad=9.8930e-01 | lr=9.51e-03
Iter    3: loss=1.486949e-01 | grad=2.7895e+00 | lr=9.51e-03
Iter    4: loss=6.599277e-02 | grad=1.8550e+00 | lr=9.51e-03
Iter    5: loss=1.362727e-01 | grad=3.2043e+00 | lr=9.51e-03
Iter    6: loss=1.163699e-01 | grad=4.2225e+00 | lr=9.51e-03
Iter    7: loss=8.594134e-02 | grad=7.2144e-01 | lr=9.51e-03
Iter    8: loss=9.180199e-02 | grad=3.2117e+00 | lr=9.51e-03
Iter    9: loss=9.916110e-02 | grad=2.8487e+00 | lr=9.51e-03
Iter  100: loss=1.746104e-02 | grad=5.4787e-01 | lr=4.76e-03
Iter  200: loss=8.124440e-03 | grad=5.6798e-01 | lr=1.19e-03
Iter  300: loss=7.354391e-03 | grad=9.8335e-02 | lr=2.97e-04
Iter  400: loss=6.982153e-03 | grad=4.6324e-02 | lr=7.43e-05
✓ Saved loss history for timestep 40 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_40.json

 Final loss: 7.164181e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=39/50 (t=0.780)
======================================================================

============================================================
Training timestep n=39 (BACKWARD STEP)
Time: t=0.780, Future models: 11
============================================================
Iter    0: loss=1.026274e-02 | grad=2.2486e+00 | lr=9.46e-03
Iter    1: loss=3.622016e-01 | grad=4.3863e+00 | lr=9.46e-03
Iter    2: loss=2.339525e-01 | grad=4.4453e+00 | lr=9.46e-03
Iter    3: loss=9.125075e-02 | grad=9.4274e-01 | lr=9.46e-03
Iter    4: loss=1.766309e-01 | grad=4.2151e+00 | lr=9.46e-03
Iter    5: loss=1.802293e-01 | grad=4.5532e+00 | lr=9.46e-03
Iter    6: loss=1.149139e-01 | grad=2.8669e+00 | lr=9.46e-03
Iter    7: loss=7.254728e-02 | grad=1.6163e+00 | lr=9.46e-03
Iter    8: loss=1.336189e-01 | grad=3.6887e+00 | lr=9.46e-03
Iter    9: loss=1.314174e-01 | grad=3.9761e+00 | lr=9.46e-03
Iter  100: loss=8.263870e-03 | grad=1.4128e+00 | lr=4.73e-03
Iter  200: loss=5.677796e-03 | grad=9.4122e-01 | lr=2.37e-03
Iter  300: loss=5.287450e-03 | grad=2.0361e+00 | lr=2.37e-03
Iter  400: loss=3.470561e-03 | grad=8.6958e-01 | lr=1.18e-03
✓ Saved loss history for timestep 39 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_39.json

 Final loss: 2.905656e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=38/50 (t=0.760)
======================================================================

============================================================
Training timestep n=38 (BACKWARD STEP)
Time: t=0.760, Future models: 12
============================================================
Iter    0: loss=7.146413e-03 | grad=2.0088e+00 | lr=9.42e-03
Iter    1: loss=1.981476e-01 | grad=3.0905e+00 | lr=9.42e-03
Iter    2: loss=9.351287e-02 | grad=6.6659e-01 | lr=9.42e-03
Iter    3: loss=1.699521e-01 | grad=3.8872e+00 | lr=9.42e-03
Iter    4: loss=9.565695e-02 | grad=2.5753e+00 | lr=9.42e-03
Iter    5: loss=1.065016e-01 | grad=3.6847e+00 | lr=9.42e-03
Iter    6: loss=1.131232e-01 | grad=3.8728e+00 | lr=9.42e-03
Iter    7: loss=5.361866e-02 | grad=1.2335e+00 | lr=9.42e-03
Iter    8: loss=8.049115e-02 | grad=2.5647e+00 | lr=9.42e-03
Iter    9: loss=9.042200e-02 | grad=3.3057e+00 | lr=9.42e-03
Iter  100: loss=1.866794e-02 | grad=2.4397e+00 | lr=4.71e-03
Iter  200: loss=7.990700e-03 | grad=3.6683e-01 | lr=1.18e-03
Iter  300: loss=7.496831e-03 | grad=3.6517e-02 | lr=2.94e-04
Iter  400: loss=7.131332e-03 | grad=4.4994e-02 | lr=1.47e-04
✓ Saved loss history for timestep 38 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_38.json

 Final loss: 7.022317e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=37/50 (t=0.740)
======================================================================

============================================================
Training timestep n=37 (BACKWARD STEP)
Time: t=0.740, Future models: 13
============================================================
Iter    0: loss=1.061089e-02 | grad=2.3850e+00 | lr=9.37e-03
Iter    1: loss=3.593363e-01 | grad=4.4138e+00 | lr=9.37e-03
Iter    2: loss=2.571752e-01 | grad=3.8869e+00 | lr=9.37e-03
Iter    3: loss=8.875503e-02 | grad=1.7950e+00 | lr=9.37e-03
Iter    4: loss=1.532752e-01 | grad=4.3556e+00 | lr=9.37e-03
Iter    5: loss=1.613896e-01 | grad=4.3544e+00 | lr=9.37e-03
Iter    6: loss=8.404391e-02 | grad=3.2126e+00 | lr=9.37e-03
Iter    7: loss=6.250267e-02 | grad=3.0182e+00 | lr=9.37e-03
Iter    8: loss=1.057796e-01 | grad=4.4653e+00 | lr=9.37e-03
Iter    9: loss=7.651522e-02 | grad=3.9620e+00 | lr=9.37e-03
Iter  100: loss=2.320351e-02 | grad=3.3423e+00 | lr=9.37e-03
Iter  200: loss=3.710374e-03 | grad=1.0791e+00 | lr=4.68e-03
Iter  300: loss=2.918871e-03 | grad=1.4331e-01 | lr=2.34e-03
Iter  400: loss=2.663199e-03 | grad=1.6321e-01 | lr=2.34e-03
✓ Saved loss history for timestep 37 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_37.json

 Final loss: 2.623040e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=36/50 (t=0.720)
======================================================================

============================================================
Training timestep n=36 (BACKWARD STEP)
Time: t=0.720, Future models: 14
============================================================
Iter    0: loss=7.789812e-03 | grad=2.0956e+00 | lr=9.32e-03
Iter    1: loss=1.617119e-01 | grad=3.5123e+00 | lr=9.32e-03
Iter    2: loss=7.945757e-02 | grad=6.1958e-01 | lr=9.32e-03
Iter    3: loss=1.993448e-01 | grad=1.8158e+00 | lr=9.32e-03
Iter    4: loss=9.106658e-02 | grad=2.5843e+00 | lr=9.32e-03
Iter    5: loss=8.832543e-02 | grad=2.8341e+00 | lr=9.32e-03
Iter    6: loss=7.937716e-02 | grad=3.7256e+00 | lr=9.32e-03
Iter    7: loss=7.016367e-02 | grad=1.2679e+00 | lr=9.32e-03
Iter    8: loss=7.687261e-02 | grad=3.2121e+00 | lr=9.32e-03
Iter    9: loss=7.866150e-02 | grad=1.7233e+00 | lr=9.32e-03
Iter  100: loss=9.892983e-03 | grad=2.6322e-01 | lr=4.66e-03
Iter  200: loss=7.136284e-03 | grad=1.6803e+00 | lr=2.33e-03
Iter  300: loss=5.505640e-03 | grad=2.4642e-01 | lr=2.33e-03
Iter  400: loss=6.453516e-03 | grad=3.0051e+00 | lr=2.33e-03
✓ Saved loss history for timestep 36 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_36.json

 Final loss: 3.760966e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=35/50 (t=0.700)
======================================================================

============================================================
Training timestep n=35 (BACKWARD STEP)
Time: t=0.700, Future models: 15
============================================================
Iter    0: loss=5.878765e-03 | grad=2.0182e+00 | lr=9.28e-03
Iter    1: loss=2.656104e-01 | grad=4.2515e+00 | lr=9.28e-03
Iter    2: loss=1.637425e-01 | grad=1.7435e+00 | lr=9.28e-03
Iter    3: loss=1.866997e-01 | grad=3.3702e+00 | lr=9.28e-03
Iter    4: loss=1.665086e-01 | grad=3.5884e+00 | lr=9.28e-03
Iter    5: loss=5.593131e-02 | grad=2.1859e+00 | lr=9.28e-03
Iter    6: loss=1.376282e-01 | grad=3.8590e+00 | lr=9.28e-03
Iter    7: loss=1.552277e-01 | grad=4.3977e+00 | lr=9.28e-03
Iter    8: loss=9.303857e-02 | grad=4.2346e+00 | lr=9.28e-03
Iter    9: loss=6.911206e-02 | grad=1.9636e+00 | lr=9.28e-03
Iter  100: loss=6.220139e-03 | grad=1.0551e+00 | lr=4.64e-03
Iter  200: loss=5.265973e-03 | grad=1.1121e+00 | lr=2.32e-03
Iter  300: loss=4.891158e-03 | grad=1.6852e-01 | lr=1.16e-03
Iter  400: loss=4.824379e-03 | grad=4.2086e-01 | lr=1.16e-03
✓ Saved loss history for timestep 35 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_35.json

 Final loss: 4.674353e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=34/50 (t=0.680)
======================================================================

============================================================
Training timestep n=34 (BACKWARD STEP)
Time: t=0.680, Future models: 16
============================================================
Iter    0: loss=9.412248e-03 | grad=2.5925e+00 | lr=9.23e-03
Iter    1: loss=2.461216e-01 | grad=3.5891e+00 | lr=9.23e-03
Iter    2: loss=1.478321e-01 | grad=3.4805e+00 | lr=9.23e-03
Iter    3: loss=9.506465e-02 | grad=2.0931e+00 | lr=9.23e-03
Iter    4: loss=1.068265e-01 | grad=4.2960e+00 | lr=9.23e-03
Iter    5: loss=9.534964e-02 | grad=1.3045e+00 | lr=9.23e-03
Iter    6: loss=8.424470e-02 | grad=4.7374e+00 | lr=9.23e-03
Iter    7: loss=9.602608e-02 | grad=1.3517e+00 | lr=9.23e-03
Iter    8: loss=7.252046e-02 | grad=1.3528e+00 | lr=9.23e-03
Iter    9: loss=5.966062e-02 | grad=2.8367e+00 | lr=9.23e-03
Iter  100: loss=1.332825e-02 | grad=3.3022e+00 | lr=4.61e-03
Iter  200: loss=1.006722e-02 | grad=2.3947e+00 | lr=4.61e-03
Iter  300: loss=3.409291e-03 | grad=6.2208e-01 | lr=2.31e-03
Iter  400: loss=3.813007e-03 | grad=1.7796e+00 | lr=2.31e-03
✓ Saved loss history for timestep 34 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_34.json

 Final loss: 3.010926e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=33/50 (t=0.660)
======================================================================

============================================================
Training timestep n=33 (BACKWARD STEP)
Time: t=0.660, Future models: 17
============================================================
Iter    0: loss=7.672731e-03 | grad=2.1996e+00 | lr=9.18e-03
Iter    1: loss=2.143815e-01 | grad=4.0421e+00 | lr=9.18e-03
Iter    2: loss=2.326132e-01 | grad=1.3612e+00 | lr=9.18e-03
Iter    3: loss=1.496236e-01 | grad=4.1299e+00 | lr=9.18e-03
Iter    4: loss=9.701104e-02 | grad=3.0438e+00 | lr=9.18e-03
Iter    5: loss=1.076932e-01 | grad=1.5881e+00 | lr=9.18e-03
Iter    6: loss=1.353683e-01 | grad=4.7812e+00 | lr=9.18e-03
Iter    7: loss=9.920618e-02 | grad=3.9756e+00 | lr=9.18e-03
Iter    8: loss=5.412248e-02 | grad=4.9827e-01 | lr=9.18e-03
Iter    9: loss=8.987617e-02 | grad=3.8490e+00 | lr=9.18e-03
Iter  100: loss=1.988029e-02 | grad=2.3866e+00 | lr=4.59e-03
Iter  200: loss=7.029697e-03 | grad=9.7310e-02 | lr=1.15e-03
Iter  300: loss=5.651740e-03 | grad=3.6775e-01 | lr=1.15e-03
Iter  400: loss=5.053071e-03 | grad=4.8314e-01 | lr=1.15e-03
✓ Saved loss history for timestep 33 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_33.json

 Final loss: 4.566881e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=32/50 (t=0.640)
======================================================================

============================================================
Training timestep n=32 (BACKWARD STEP)
Time: t=0.640, Future models: 18
============================================================
Iter    0: loss=1.004225e-02 | grad=2.5372e+00 | lr=9.14e-03
Iter    1: loss=3.458430e-01 | grad=3.7335e+00 | lr=9.14e-03
Iter    2: loss=2.195559e-01 | grad=4.4585e+00 | lr=9.14e-03
Iter    3: loss=7.789272e-02 | grad=1.5569e+00 | lr=9.14e-03
Iter    4: loss=1.638281e-01 | grad=3.2319e+00 | lr=9.14e-03
Iter    5: loss=1.612084e-01 | grad=4.6025e+00 | lr=9.14e-03
Iter    6: loss=1.070162e-01 | grad=2.9295e+00 | lr=9.14e-03
Iter    7: loss=6.627195e-02 | grad=4.5598e+00 | lr=9.14e-03
Iter    8: loss=8.402727e-02 | grad=4.4790e+00 | lr=9.14e-03
Iter    9: loss=6.159326e-02 | grad=6.0269e-01 | lr=9.14e-03
Iter  100: loss=7.959906e-03 | grad=2.1157e+00 | lr=4.57e-03
Iter  200: loss=5.040105e-03 | grad=9.0751e-01 | lr=4.57e-03
Iter  300: loss=5.751975e-03 | grad=2.9497e+00 | lr=2.28e-03
Iter  400: loss=3.534414e-03 | grad=1.5017e+00 | lr=2.28e-03
✓ Saved loss history for timestep 32 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_32.json

 Final loss: 3.094035e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=31/50 (t=0.620)
======================================================================

============================================================
Training timestep n=31 (BACKWARD STEP)
Time: t=0.620, Future models: 19
============================================================
Iter    0: loss=8.027390e-03 | grad=2.4516e+00 | lr=9.09e-03
Iter    1: loss=2.715025e-01 | grad=4.5486e+00 | lr=9.09e-03
Iter    2: loss=1.440059e-01 | grad=3.0959e+00 | lr=9.09e-03
Iter    3: loss=1.092091e-01 | grad=3.9539e+00 | lr=9.09e-03
Iter    4: loss=1.563237e-01 | grad=2.7538e+00 | lr=9.09e-03
Iter    5: loss=8.014403e-02 | grad=2.1590e+00 | lr=9.09e-03
Iter    6: loss=7.878360e-02 | grad=3.7724e+00 | lr=9.09e-03
Iter    7: loss=8.483420e-02 | grad=3.8910e+00 | lr=9.09e-03
Iter    8: loss=4.096124e-02 | grad=5.5185e-01 | lr=9.09e-03
Iter    9: loss=7.697641e-02 | grad=2.8005e+00 | lr=9.09e-03
Iter  100: loss=9.275654e-03 | grad=4.4466e-01 | lr=4.55e-03
Iter  200: loss=5.853076e-03 | grad=1.4600e+00 | lr=4.55e-03
Iter  300: loss=3.395609e-03 | grad=7.3599e-01 | lr=2.27e-03
Iter  400: loss=3.529003e-03 | grad=2.1706e+00 | lr=2.27e-03
✓ Saved loss history for timestep 31 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_31.json

 Final loss: 2.966330e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=30/50 (t=0.600)
======================================================================

============================================================
Training timestep n=30 (BACKWARD STEP)
Time: t=0.600, Future models: 20
============================================================
Iter    0: loss=7.293711e-03 | grad=2.4869e+00 | lr=9.05e-03
Iter    1: loss=2.175621e-01 | grad=4.5506e+00 | lr=9.05e-03
Iter    2: loss=7.853197e-02 | grad=1.6159e+00 | lr=9.05e-03
Iter    3: loss=1.970222e-01 | grad=4.4911e+00 | lr=9.05e-03
Iter    4: loss=1.597144e-01 | grad=4.8546e+00 | lr=9.05e-03
Iter    5: loss=7.990270e-02 | grad=1.1911e+00 | lr=9.05e-03
Iter    6: loss=1.097818e-01 | grad=4.3625e+00 | lr=9.05e-03
Iter    7: loss=1.341557e-01 | grad=4.0942e+00 | lr=9.05e-03
Iter    8: loss=8.335615e-02 | grad=3.6003e+00 | lr=9.05e-03
Iter    9: loss=4.732881e-02 | grad=1.0188e+00 | lr=9.05e-03
Iter  100: loss=1.247648e-02 | grad=2.5462e+00 | lr=4.52e-03
Iter  200: loss=6.527349e-03 | grad=1.9624e+00 | lr=2.26e-03
Iter  300: loss=4.655048e-03 | grad=9.8031e-01 | lr=2.26e-03
Iter  400: loss=6.732017e-03 | grad=3.1511e+00 | lr=1.13e-03
✓ Saved loss history for timestep 30 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_30.json

 Final loss: 3.734691e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=29/50 (t=0.580)
======================================================================

============================================================
Training timestep n=29 (BACKWARD STEP)
Time: t=0.580, Future models: 21
============================================================
Iter    0: loss=8.141069e-03 | grad=2.6044e+00 | lr=9.00e-03
Iter    1: loss=2.747863e-01 | grad=4.2059e+00 | lr=9.00e-03
Iter    2: loss=1.487132e-01 | grad=4.1491e+00 | lr=9.00e-03
Iter    3: loss=8.618163e-02 | grad=2.7733e+00 | lr=9.00e-03
Iter    4: loss=1.128896e-01 | grad=4.3545e+00 | lr=9.00e-03
Iter    5: loss=4.888879e-02 | grad=1.7248e+00 | lr=9.00e-03
Iter    6: loss=1.050243e-01 | grad=5.1243e+00 | lr=9.00e-03
Iter    7: loss=9.836969e-02 | grad=3.3508e+00 | lr=9.00e-03
Iter    8: loss=4.063565e-02 | grad=6.9955e-01 | lr=9.00e-03
Iter    9: loss=7.370089e-02 | grad=3.1550e+00 | lr=9.00e-03
Iter  100: loss=1.501129e-02 | grad=2.7145e+00 | lr=4.50e-03
Iter  200: loss=5.579520e-03 | grad=1.3770e+00 | lr=2.25e-03
Iter  300: loss=5.281119e-03 | grad=1.8206e+00 | lr=2.25e-03
Iter  400: loss=4.351399e-03 | grad=4.0129e-01 | lr=1.13e-03
✓ Saved loss history for timestep 29 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_29.json

 Final loss: 3.283698e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=28/50 (t=0.560)
======================================================================

============================================================
Training timestep n=28 (BACKWARD STEP)
Time: t=0.560, Future models: 22
============================================================
Iter    0: loss=6.958949e-03 | grad=2.5566e+00 | lr=8.96e-03
Iter    1: loss=3.300085e-01 | grad=5.1460e+00 | lr=8.96e-03
Iter    2: loss=1.892173e-01 | grad=4.7489e+00 | lr=8.96e-03
Iter    3: loss=5.079631e-02 | grad=3.0315e+00 | lr=8.96e-03
Iter    4: loss=2.028373e-01 | grad=2.2354e+00 | lr=8.96e-03
Iter    5: loss=1.121176e-01 | grad=4.7642e+00 | lr=8.96e-03
Iter    6: loss=8.569169e-02 | grad=1.3413e+00 | lr=8.96e-03
Iter    7: loss=8.488175e-02 | grad=5.3647e+00 | lr=8.96e-03
Iter    8: loss=7.636563e-02 | grad=4.0223e+00 | lr=8.96e-03
Iter    9: loss=4.263632e-02 | grad=9.9639e-01 | lr=8.96e-03
Iter  100: loss=9.812459e-03 | grad=2.7475e+00 | lr=4.48e-03
Iter  200: loss=7.740743e-03 | grad=1.5824e+00 | lr=2.24e-03
Iter  300: loss=4.273287e-03 | grad=5.4447e-01 | lr=2.24e-03
Iter  400: loss=3.476861e-03 | grad=4.9958e-01 | lr=2.24e-03
✓ Saved loss history for timestep 28 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_28.json

 Final loss: 4.027474e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=27/50 (t=0.540)
======================================================================

============================================================
Training timestep n=27 (BACKWARD STEP)
Time: t=0.540, Future models: 23
============================================================
Iter    0: loss=4.310500e-03 | grad=1.0340e+00 | lr=8.91e-03
Iter    1: loss=2.567725e-01 | grad=4.7742e+00 | lr=8.91e-03
Iter    2: loss=7.431158e-02 | grad=1.8410e+00 | lr=8.91e-03
Iter    3: loss=2.163565e-01 | grad=4.6515e+00 | lr=8.91e-03
Iter    4: loss=1.843349e-01 | grad=5.3483e+00 | lr=8.91e-03
Iter    5: loss=5.714869e-02 | grad=3.6812e+00 | lr=8.91e-03
Iter    6: loss=1.389772e-01 | grad=5.0093e+00 | lr=8.91e-03
Iter    7: loss=1.813830e-01 | grad=5.4431e+00 | lr=8.91e-03
Iter    8: loss=1.398060e-01 | grad=4.5693e+00 | lr=8.91e-03
Iter    9: loss=5.993518e-02 | grad=1.8530e+00 | lr=8.91e-03
Iter  100: loss=9.585271e-03 | grad=7.5441e-01 | lr=4.46e-03
Iter  200: loss=3.959598e-03 | grad=1.5877e-01 | lr=1.11e-03
Iter  300: loss=3.430632e-03 | grad=4.4500e-01 | lr=1.11e-03
Iter  400: loss=3.344210e-03 | grad=5.7198e-01 | lr=1.11e-03
✓ Saved loss history for timestep 27 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_27.json

 Final loss: 3.134070e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=26/50 (t=0.520)
======================================================================

============================================================
Training timestep n=26 (BACKWARD STEP)
Time: t=0.520, Future models: 24
============================================================
Iter    0: loss=5.434865e-03 | grad=2.1375e+00 | lr=8.87e-03
Iter    1: loss=3.032609e-01 | grad=4.5176e+00 | lr=8.87e-03
Iter    2: loss=1.674601e-01 | grad=4.0208e+00 | lr=8.87e-03
Iter    3: loss=9.080200e-02 | grad=1.8558e+00 | lr=8.87e-03
Iter    4: loss=1.238208e-01 | grad=4.7916e+00 | lr=8.87e-03
Iter    5: loss=7.579052e-02 | grad=3.3513e+00 | lr=8.87e-03
Iter    6: loss=7.976578e-02 | grad=2.4935e+00 | lr=8.87e-03
Iter    7: loss=1.171341e-01 | grad=5.0457e+00 | lr=8.87e-03
Iter    8: loss=1.004168e-01 | grad=3.0164e+00 | lr=8.87e-03
Iter    9: loss=4.223445e-02 | grad=8.6552e-01 | lr=8.87e-03
Iter  100: loss=6.627364e-03 | grad=1.7420e+00 | lr=4.43e-03
Iter  200: loss=4.850873e-03 | grad=9.9932e-01 | lr=2.22e-03
Iter  300: loss=4.396964e-03 | grad=2.1958e-01 | lr=1.11e-03
Iter  400: loss=4.166248e-03 | grad=3.0309e-01 | lr=1.11e-03
✓ Saved loss history for timestep 26 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_26.json

 Final loss: 4.037666e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=25/50 (t=0.500)
======================================================================

============================================================
Training timestep n=25 (BACKWARD STEP)
Time: t=0.500, Future models: 25
============================================================
Iter    0: loss=7.906136e-03 | grad=2.4514e+00 | lr=8.82e-03
Iter    1: loss=3.107750e-01 | grad=3.6910e+00 | lr=8.82e-03
Iter    2: loss=1.762534e-01 | grad=4.7988e+00 | lr=8.82e-03
Iter    3: loss=9.582194e-02 | grad=1.3494e+00 | lr=8.82e-03
Iter    4: loss=1.808617e-01 | grad=3.5664e+00 | lr=8.82e-03
Iter    5: loss=1.425894e-01 | grad=3.9548e+00 | lr=8.82e-03
Iter    6: loss=5.054570e-02 | grad=2.3742e+00 | lr=8.82e-03
Iter    7: loss=1.122348e-01 | grad=5.3185e+00 | lr=8.82e-03
Iter    8: loss=1.145290e-01 | grad=5.3444e+00 | lr=8.82e-03
Iter    9: loss=7.709591e-02 | grad=1.3883e+00 | lr=8.82e-03
Iter  100: loss=7.422575e-03 | grad=3.1071e+00 | lr=4.41e-03
Iter  200: loss=7.273220e-03 | grad=3.8951e+00 | lr=4.41e-03
Iter  300: loss=4.807815e-03 | grad=3.1377e+00 | lr=1.10e-03
Iter  400: loss=2.796831e-03 | grad=8.9467e-02 | lr=5.51e-04
✓ Saved loss history for timestep 25 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_25.json

 Final loss: 2.835838e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=24/50 (t=0.480)
======================================================================

============================================================
Training timestep n=24 (BACKWARD STEP)
Time: t=0.480, Future models: 26
============================================================
Iter    0: loss=6.983140e-03 | grad=2.3978e+00 | lr=8.78e-03
Iter    1: loss=2.709279e-01 | grad=4.8494e+00 | lr=8.78e-03
Iter    2: loss=2.271996e-01 | grad=2.4830e+00 | lr=8.78e-03
Iter    3: loss=9.511764e-02 | grad=1.1932e+00 | lr=8.78e-03
Iter    4: loss=1.180495e-01 | grad=4.9725e+00 | lr=8.78e-03
Iter    5: loss=7.756282e-02 | grad=2.8380e+00 | lr=8.78e-03
Iter    6: loss=7.446879e-02 | grad=5.0838e+00 | lr=8.78e-03
Iter    7: loss=7.070028e-02 | grad=3.5346e+00 | lr=8.78e-03
Iter    8: loss=3.308977e-02 | grad=1.5041e+00 | lr=8.78e-03
Iter    9: loss=6.086360e-02 | grad=3.2062e+00 | lr=8.78e-03
Iter  100: loss=7.821960e-03 | grad=3.5444e+00 | lr=4.39e-03
Iter  200: loss=5.042729e-03 | grad=1.6233e+00 | lr=4.39e-03
Iter  300: loss=3.714905e-03 | grad=7.9882e-01 | lr=2.19e-03
Iter  400: loss=3.620815e-03 | grad=1.6052e+00 | lr=2.19e-03
✓ Saved loss history for timestep 24 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_24.json

 Final loss: 3.467373e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=23/50 (t=0.460)
======================================================================

============================================================
Training timestep n=23 (BACKWARD STEP)
Time: t=0.460, Future models: 27
============================================================
Iter    0: loss=7.765885e-03 | grad=2.6137e+00 | lr=8.73e-03
Iter    1: loss=2.668424e-01 | grad=4.2720e+00 | lr=8.73e-03
Iter    2: loss=1.379100e-01 | grad=4.2021e+00 | lr=8.73e-03
Iter    3: loss=1.246145e-01 | grad=3.4533e+00 | lr=8.73e-03
Iter    4: loss=1.474710e-01 | grad=5.1350e+00 | lr=8.73e-03
Iter    5: loss=8.334164e-02 | grad=3.6899e+00 | lr=8.73e-03
Iter    6: loss=7.271227e-02 | grad=2.8509e+00 | lr=8.73e-03
Iter    7: loss=1.092972e-01 | grad=5.2035e+00 | lr=8.73e-03
Iter    8: loss=7.587426e-02 | grad=5.0169e+00 | lr=8.73e-03
Iter    9: loss=4.354852e-02 | grad=1.3405e+00 | lr=8.73e-03
Iter  100: loss=7.296672e-03 | grad=3.1702e+00 | lr=4.37e-03
Iter  200: loss=3.305444e-03 | grad=3.8917e-01 | lr=2.18e-03
Iter  300: loss=3.063693e-03 | grad=1.0519e+00 | lr=2.18e-03
Iter  400: loss=2.792918e-03 | grad=6.2365e-01 | lr=1.09e-03
✓ Saved loss history for timestep 23 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_23.json

 Final loss: 2.610370e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=22/50 (t=0.440)
======================================================================

============================================================
Training timestep n=22 (BACKWARD STEP)
Time: t=0.440, Future models: 28
============================================================
Iter    0: loss=6.371831e-03 | grad=2.2524e+00 | lr=8.69e-03
Iter    1: loss=2.240660e-01 | grad=4.6806e+00 | lr=8.69e-03
Iter    2: loss=1.330611e-01 | grad=2.2338e+00 | lr=8.69e-03
Iter    3: loss=2.529272e-01 | grad=2.0760e+00 | lr=8.69e-03
Iter    4: loss=1.726035e-01 | grad=5.1244e+00 | lr=8.69e-03
Iter    5: loss=1.189677e-01 | grad=3.3084e+00 | lr=8.69e-03
Iter    6: loss=6.882507e-02 | grad=1.9083e+00 | lr=8.69e-03
Iter    7: loss=1.224522e-01 | grad=5.2186e+00 | lr=8.69e-03
Iter    8: loss=1.032357e-01 | grad=3.7287e+00 | lr=8.69e-03
Iter    9: loss=5.256849e-02 | grad=1.0063e+00 | lr=8.69e-03
Iter  100: loss=9.894802e-03 | grad=4.7841e-01 | lr=4.35e-03
Iter  200: loss=4.677684e-03 | grad=2.2051e+00 | lr=2.17e-03
Iter  300: loss=3.968973e-03 | grad=1.8932e+00 | lr=2.17e-03
Iter  400: loss=3.209607e-03 | grad=1.3996e+00 | lr=2.17e-03
✓ Saved loss history for timestep 22 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_22.json

 Final loss: 3.597307e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=21/50 (t=0.420)
======================================================================

============================================================
Training timestep n=21 (BACKWARD STEP)
Time: t=0.420, Future models: 29
============================================================
Iter    0: loss=8.740553e-03 | grad=3.4690e+00 | lr=8.65e-03
Iter    1: loss=2.918452e-01 | grad=4.8912e+00 | lr=8.65e-03
Iter    2: loss=2.343557e-01 | grad=3.5625e+00 | lr=8.65e-03
Iter    3: loss=1.110031e-01 | grad=1.9087e+00 | lr=8.65e-03
Iter    4: loss=1.084218e-01 | grad=3.7109e+00 | lr=8.65e-03
Iter    5: loss=1.203005e-01 | grad=3.8454e+00 | lr=8.65e-03
Iter    6: loss=4.938883e-02 | grad=2.9985e+00 | lr=8.65e-03
Iter    7: loss=9.436741e-02 | grad=4.6935e+00 | lr=8.65e-03
Iter    8: loss=1.112996e-01 | grad=4.4925e+00 | lr=8.65e-03
Iter    9: loss=5.550921e-02 | grad=3.4229e+00 | lr=8.65e-03
Iter  100: loss=9.403478e-03 | grad=3.1018e+00 | lr=4.32e-03
Iter  200: loss=6.307962e-03 | grad=2.5346e+00 | lr=2.16e-03
Iter  300: loss=3.981246e-03 | grad=1.4157e+00 | lr=2.16e-03
Iter  400: loss=3.312222e-03 | grad=1.8118e+00 | lr=2.16e-03
✓ Saved loss history for timestep 21 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_21.json

 Final loss: 2.696508e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=20/50 (t=0.400)
======================================================================

============================================================
Training timestep n=20 (BACKWARD STEP)
Time: t=0.400, Future models: 30
============================================================
Iter    0: loss=6.366473e-03 | grad=2.6599e+00 | lr=8.60e-03
Iter    1: loss=2.730455e-01 | grad=4.9274e+00 | lr=8.60e-03
Iter    2: loss=1.717394e-01 | grad=4.0212e+00 | lr=8.60e-03
Iter    3: loss=7.692513e-02 | grad=2.1655e+00 | lr=8.60e-03
Iter    4: loss=3.402422e-01 | grad=2.2855e+00 | lr=8.60e-03
Iter    5: loss=1.470661e-01 | grad=3.7926e+00 | lr=8.60e-03
Iter    6: loss=1.438120e-01 | grad=1.9065e+00 | lr=8.60e-03
Iter    7: loss=1.222882e-01 | grad=1.6003e+00 | lr=8.60e-03
Iter    8: loss=1.018164e-01 | grad=4.9495e+00 | lr=8.60e-03
Iter    9: loss=6.548308e-02 | grad=3.1290e+00 | lr=8.60e-03
Iter  100: loss=1.143378e-02 | grad=4.3231e+00 | lr=4.30e-03
Iter  200: loss=3.722475e-03 | grad=1.6373e+00 | lr=2.15e-03
Iter  300: loss=2.845590e-03 | grad=4.8167e-01 | lr=2.15e-03
Iter  400: loss=3.535248e-03 | grad=2.8212e+00 | lr=2.15e-03
✓ Saved loss history for timestep 20 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_20.json

 Final loss: 2.735776e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=19/50 (t=0.380)
======================================================================

============================================================
Training timestep n=19 (BACKWARD STEP)
Time: t=0.380, Future models: 31
============================================================
Iter    0: loss=6.611749e-03 | grad=2.5800e+00 | lr=8.56e-03
Iter    1: loss=2.551386e-01 | grad=4.8697e+00 | lr=8.56e-03
Iter    2: loss=1.847318e-01 | grad=2.3482e+00 | lr=8.56e-03
Iter    3: loss=1.260467e-01 | grad=3.5142e+00 | lr=8.56e-03
Iter    4: loss=1.098421e-01 | grad=4.2294e+00 | lr=8.56e-03
Iter    5: loss=6.984977e-02 | grad=1.1438e+00 | lr=8.56e-03
Iter    6: loss=9.212568e-02 | grad=4.6727e+00 | lr=8.56e-03
Iter    7: loss=8.529890e-02 | grad=2.6456e+00 | lr=8.56e-03
Iter    8: loss=3.945453e-02 | grad=1.2391e+00 | lr=8.56e-03
Iter    9: loss=5.724169e-02 | grad=3.7163e+00 | lr=8.56e-03
Iter  100: loss=6.012048e-03 | grad=2.3057e+00 | lr=4.28e-03
Iter  200: loss=3.526018e-03 | grad=2.5423e-01 | lr=2.14e-03
Iter  300: loss=3.515096e-03 | grad=1.5863e+00 | lr=2.14e-03
Iter  400: loss=2.635439e-03 | grad=3.8966e-01 | lr=5.35e-04
✓ Saved loss history for timestep 19 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_19.json

 Final loss: 2.583738e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=18/50 (t=0.360)
======================================================================

============================================================
Training timestep n=18 (BACKWARD STEP)
Time: t=0.360, Future models: 32
============================================================
Iter    0: loss=6.626410e-03 | grad=2.5182e+00 | lr=8.52e-03
Iter    1: loss=2.846825e-01 | grad=4.9140e+00 | lr=8.52e-03
Iter    2: loss=1.641826e-01 | grad=4.9248e+00 | lr=8.52e-03
Iter    3: loss=1.032271e-01 | grad=1.8843e+00 | lr=8.52e-03
Iter    4: loss=2.062879e-01 | grad=2.6328e+00 | lr=8.52e-03
Iter    5: loss=1.667524e-01 | grad=4.0096e+00 | lr=8.52e-03
Iter    6: loss=1.438596e-01 | grad=2.7135e+00 | lr=8.52e-03
Iter    7: loss=4.946681e-02 | grad=2.1250e+00 | lr=8.52e-03
Iter    8: loss=1.393767e-01 | grad=2.6730e+00 | lr=8.52e-03
Iter    9: loss=1.361446e-01 | grad=2.5746e+00 | lr=8.52e-03
Iter  100: loss=7.569189e-03 | grad=2.3656e+00 | lr=4.26e-03
Iter  200: loss=8.679574e-03 | grad=4.0584e+00 | lr=2.13e-03
Iter  300: loss=2.980003e-03 | grad=2.1763e-01 | lr=1.06e-03
Iter  400: loss=2.802377e-03 | grad=3.6847e-01 | lr=1.06e-03
✓ Saved loss history for timestep 18 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_18.json

 Final loss: 2.626388e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=17/50 (t=0.340)
======================================================================

============================================================
Training timestep n=17 (BACKWARD STEP)
Time: t=0.340, Future models: 33
============================================================
Iter    0: loss=6.492907e-03 | grad=2.7346e+00 | lr=8.48e-03
Iter    1: loss=2.780941e-01 | grad=5.0560e+00 | lr=8.48e-03
Iter    2: loss=1.717949e-01 | grad=4.0091e+00 | lr=8.48e-03
Iter    3: loss=1.254952e-01 | grad=1.7285e+00 | lr=8.48e-03
Iter    4: loss=1.532198e-01 | grad=4.1059e+00 | lr=8.48e-03
Iter    5: loss=1.517033e-01 | grad=3.2856e+00 | lr=8.48e-03
Iter    6: loss=6.994106e-02 | grad=2.9724e+00 | lr=8.48e-03
Iter    7: loss=7.524011e-02 | grad=3.7081e+00 | lr=8.48e-03
Iter    8: loss=9.642372e-02 | grad=4.7189e+00 | lr=8.48e-03
Iter    9: loss=5.901574e-02 | grad=3.3669e+00 | lr=8.48e-03
Iter  100: loss=7.199188e-03 | grad=9.6964e-01 | lr=4.24e-03
Iter  200: loss=3.334194e-03 | grad=7.9431e-01 | lr=2.12e-03
Iter  300: loss=2.929297e-03 | grad=2.4982e-01 | lr=2.12e-03
Iter  400: loss=2.972319e-03 | grad=2.0044e-01 | lr=1.06e-03
✓ Saved loss history for timestep 17 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_17.json

 Final loss: 2.808487e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=16/50 (t=0.320)
======================================================================

============================================================
Training timestep n=16 (BACKWARD STEP)
Time: t=0.320, Future models: 34
============================================================
Iter    0: loss=6.658012e-03 | grad=2.3768e+00 | lr=8.43e-03
Iter    1: loss=1.993543e-01 | grad=4.6899e+00 | lr=8.43e-03
Iter    2: loss=1.214943e-01 | grad=1.9840e+00 | lr=8.43e-03
Iter    3: loss=1.503613e-01 | grad=3.6084e+00 | lr=8.43e-03
Iter    4: loss=1.188727e-01 | grad=4.2232e+00 | lr=8.43e-03
Iter    5: loss=6.922580e-02 | grad=1.2603e+00 | lr=8.43e-03
Iter    6: loss=1.116114e-01 | grad=4.5281e+00 | lr=8.43e-03
Iter    7: loss=1.218662e-01 | grad=3.3812e+00 | lr=8.43e-03
Iter    8: loss=5.866474e-02 | grad=3.3919e+00 | lr=8.43e-03
Iter    9: loss=7.280660e-02 | grad=2.3652e+00 | lr=8.43e-03
Iter  100: loss=3.909549e-03 | grad=1.3009e+00 | lr=4.22e-03
Iter  200: loss=2.958622e-03 | grad=1.1127e+00 | lr=2.11e-03
Iter  300: loss=2.631812e-03 | grad=5.0398e-01 | lr=2.11e-03
Iter  400: loss=2.461850e-03 | grad=3.0433e-01 | lr=1.05e-03
✓ Saved loss history for timestep 16 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_16.json

 Final loss: 2.478471e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=15/50 (t=0.300)
======================================================================

============================================================
Training timestep n=15 (BACKWARD STEP)
Time: t=0.300, Future models: 35
============================================================
Iter    0: loss=6.179785e-03 | grad=2.3666e+00 | lr=8.39e-03
Iter    1: loss=2.043189e-01 | grad=4.6256e+00 | lr=8.39e-03
Iter    2: loss=9.945308e-02 | grad=3.2435e+00 | lr=8.39e-03
Iter    3: loss=1.271542e-01 | grad=3.6541e+00 | lr=8.39e-03
Iter    4: loss=1.762241e-01 | grad=2.9507e+00 | lr=8.39e-03
Iter    5: loss=8.855958e-02 | grad=3.5072e+00 | lr=8.39e-03
Iter    6: loss=7.130826e-02 | grad=1.7416e+00 | lr=8.39e-03
Iter    7: loss=9.570532e-02 | grad=4.9894e+00 | lr=8.39e-03
Iter    8: loss=7.636982e-02 | grad=2.8138e+00 | lr=8.39e-03
Iter    9: loss=3.962984e-02 | grad=1.6312e+00 | lr=8.39e-03
Iter  100: loss=5.052943e-03 | grad=1.6933e+00 | lr=4.20e-03
Iter  200: loss=5.186690e-03 | grad=2.7535e+00 | lr=4.20e-03
Iter  300: loss=2.803570e-03 | grad=4.4350e-01 | lr=2.10e-03
Iter  400: loss=2.728941e-03 | grad=1.0312e+00 | lr=2.10e-03
✓ Saved loss history for timestep 15 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_15.json

 Final loss: 2.475346e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=14/50 (t=0.280)
======================================================================

============================================================
Training timestep n=14 (BACKWARD STEP)
Time: t=0.280, Future models: 36
============================================================
Iter    0: loss=6.905459e-03 | grad=2.3954e+00 | lr=8.35e-03
Iter    1: loss=2.110543e-01 | grad=4.4420e+00 | lr=8.35e-03
Iter    2: loss=9.670697e-02 | grad=4.3138e+00 | lr=8.35e-03
Iter    3: loss=1.689401e-01 | grad=2.3231e+00 | lr=8.35e-03
Iter    4: loss=1.790557e-01 | grad=4.0539e+00 | lr=8.35e-03
Iter    5: loss=1.250509e-01 | grad=3.8002e+00 | lr=8.35e-03
Iter    6: loss=1.084824e-01 | grad=1.6056e+00 | lr=8.35e-03
Iter    7: loss=1.100706e-01 | grad=5.1991e+00 | lr=8.35e-03
Iter    8: loss=9.210551e-02 | grad=4.8418e+00 | lr=8.35e-03
Iter    9: loss=6.577651e-02 | grad=1.0114e+00 | lr=8.35e-03
Iter  100: loss=5.998852e-03 | grad=1.8358e+00 | lr=4.17e-03
Iter  200: loss=3.914155e-03 | grad=2.0615e+00 | lr=4.17e-03
Iter  300: loss=2.970697e-03 | grad=5.4698e-01 | lr=2.09e-03
Iter  400: loss=2.757010e-03 | grad=8.2576e-01 | lr=2.09e-03
✓ Saved loss history for timestep 14 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_14.json

 Final loss: 3.281928e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=13/50 (t=0.260)
======================================================================

============================================================
Training timestep n=13 (BACKWARD STEP)
Time: t=0.260, Future models: 37
============================================================
Iter    0: loss=5.442808e-03 | grad=2.1025e+00 | lr=8.31e-03
Iter    1: loss=2.187471e-01 | grad=4.6644e+00 | lr=8.31e-03
Iter    2: loss=1.417297e-01 | grad=1.9712e+00 | lr=8.31e-03
Iter    3: loss=9.587069e-02 | grad=3.2940e+00 | lr=8.31e-03
Iter    4: loss=9.268051e-02 | grad=3.5195e+00 | lr=8.31e-03
Iter    5: loss=4.274773e-02 | grad=9.6357e-01 | lr=8.31e-03
Iter    6: loss=1.000644e-01 | grad=2.9180e+00 | lr=8.31e-03
Iter    7: loss=8.598803e-02 | grad=3.9051e+00 | lr=8.31e-03
Iter    8: loss=2.951607e-02 | grad=1.0666e+00 | lr=8.31e-03
Iter    9: loss=7.942243e-02 | grad=3.1663e+00 | lr=8.31e-03
Iter  100: loss=5.616012e-03 | grad=1.8148e+00 | lr=4.15e-03
Iter  200: loss=4.155948e-03 | grad=4.7351e-01 | lr=4.15e-03
Iter  300: loss=3.301646e-03 | grad=2.1726e-01 | lr=2.08e-03
Iter  400: loss=3.185621e-03 | grad=5.0362e-01 | lr=2.08e-03
✓ Saved loss history for timestep 13 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_13.json

 Final loss: 2.744670e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=12/50 (t=0.240)
======================================================================

============================================================
Training timestep n=12 (BACKWARD STEP)
Time: t=0.240, Future models: 38
============================================================
Iter    0: loss=6.524873e-03 | grad=1.9855e+00 | lr=8.27e-03
Iter    1: loss=1.791785e-01 | grad=4.1801e+00 | lr=8.27e-03
Iter    2: loss=6.546831e-02 | grad=2.3240e+00 | lr=8.27e-03
Iter    3: loss=1.602888e-01 | grad=4.5543e+00 | lr=8.27e-03
Iter    4: loss=1.350603e-01 | grad=3.8704e+00 | lr=8.27e-03
Iter    5: loss=1.473483e-01 | grad=1.6043e+00 | lr=8.27e-03
Iter    6: loss=1.028176e-01 | grad=2.2228e+00 | lr=8.27e-03
Iter    7: loss=1.076555e-01 | grad=3.8303e+00 | lr=8.27e-03
Iter    8: loss=7.762796e-02 | grad=3.5088e+00 | lr=8.27e-03
Iter    9: loss=6.038162e-02 | grad=1.2377e+00 | lr=8.27e-03
Iter  100: loss=4.711068e-03 | grad=2.9478e-01 | lr=4.13e-03
Iter  200: loss=5.184317e-03 | grad=1.8016e+00 | lr=4.13e-03
Iter  300: loss=3.560496e-03 | grad=2.4788e-01 | lr=2.07e-03
Iter  400: loss=2.884791e-03 | grad=5.0781e-01 | lr=2.07e-03
✓ Saved loss history for timestep 12 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_12.json

 Final loss: 2.715866e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=11/50 (t=0.220)
======================================================================

============================================================
Training timestep n=11 (BACKWARD STEP)
Time: t=0.220, Future models: 39
============================================================
Iter    0: loss=6.951026e-03 | grad=1.8608e+00 | lr=8.22e-03
Iter    1: loss=1.148541e-01 | grad=2.6957e+00 | lr=8.22e-03
Iter    2: loss=6.873887e-02 | grad=1.1049e+00 | lr=8.22e-03
Iter    3: loss=9.579635e-02 | grad=2.3717e+00 | lr=8.22e-03
Iter    4: loss=2.896973e-02 | grad=9.6925e-01 | lr=8.22e-03
Iter    5: loss=1.091485e-01 | grad=2.2587e+00 | lr=8.22e-03
Iter    6: loss=7.501620e-02 | grad=3.2682e+00 | lr=8.22e-03
Iter    7: loss=5.030204e-02 | grad=1.3258e+00 | lr=8.22e-03
Iter    8: loss=5.484616e-02 | grad=3.0445e+00 | lr=8.22e-03
Iter    9: loss=3.840960e-02 | grad=1.8417e+00 | lr=8.22e-03
Iter  100: loss=4.688193e-03 | grad=1.0825e+00 | lr=4.11e-03
Iter  200: loss=3.925232e-03 | grad=1.6254e+00 | lr=4.11e-03
Iter  300: loss=3.165872e-03 | grad=4.3545e-01 | lr=2.06e-03
Iter  400: loss=2.737992e-03 | grad=5.1856e-01 | lr=2.06e-03
✓ Saved loss history for timestep 11 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_11.json

 Final loss: 2.544332e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=10/50 (t=0.200)
======================================================================

============================================================
Training timestep n=10 (BACKWARD STEP)
Time: t=0.200, Future models: 40
============================================================
Iter    0: loss=6.332470e-03 | grad=1.9222e+00 | lr=8.18e-03
Iter    1: loss=1.256329e-01 | grad=3.3579e+00 | lr=8.18e-03
Iter    2: loss=8.519855e-02 | grad=1.2703e+00 | lr=8.18e-03
Iter    3: loss=9.100085e-02 | grad=2.8031e+00 | lr=8.18e-03
Iter    4: loss=3.078628e-02 | grad=9.4061e-01 | lr=8.18e-03
Iter    5: loss=1.094258e-01 | grad=2.8996e+00 | lr=8.18e-03
Iter    6: loss=7.761227e-02 | grad=2.8556e+00 | lr=8.18e-03
Iter    7: loss=5.386347e-02 | grad=1.9125e+00 | lr=8.18e-03
Iter    8: loss=6.821344e-02 | grad=3.3196e+00 | lr=8.18e-03
Iter    9: loss=2.824071e-02 | grad=2.0860e+00 | lr=8.18e-03
Iter  100: loss=5.678771e-03 | grad=1.2722e+00 | lr=4.09e-03
Iter  200: loss=7.126150e-03 | grad=2.2873e+00 | lr=2.05e-03
Iter  300: loss=3.286789e-03 | grad=2.0711e-01 | lr=2.05e-03
Iter  400: loss=2.721560e-03 | grad=5.5324e-01 | lr=2.05e-03
✓ Saved loss history for timestep 10 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_10.json

 Final loss: 3.119706e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=9/50 (t=0.180)
======================================================================

============================================================
Training timestep n=9 (BACKWARD STEP)
Time: t=0.180, Future models: 41
============================================================
Iter    0: loss=6.475598e-03 | grad=9.0953e-01 | lr=8.14e-03
Iter    1: loss=1.623648e-01 | grad=3.9642e+00 | lr=8.14e-03
Iter    2: loss=1.127771e-01 | grad=1.5017e+00 | lr=8.14e-03
Iter    3: loss=1.765868e-01 | grad=2.5354e+00 | lr=8.14e-03
Iter    4: loss=1.016391e-01 | grad=3.9656e+00 | lr=8.14e-03
Iter    5: loss=1.079019e-01 | grad=1.4578e+00 | lr=8.14e-03
Iter    6: loss=1.115963e-01 | grad=4.3362e+00 | lr=8.14e-03
Iter    7: loss=9.155016e-02 | grad=4.2840e+00 | lr=8.14e-03
Iter    8: loss=4.608461e-02 | grad=1.0815e+00 | lr=8.14e-03
Iter    9: loss=5.538870e-02 | grad=3.6434e+00 | lr=8.14e-03
Iter  100: loss=5.768821e-03 | grad=1.9755e+00 | lr=4.07e-03
Iter  200: loss=3.056641e-03 | grad=4.9744e-01 | lr=2.04e-03
Iter  300: loss=2.808511e-03 | grad=4.6195e-01 | lr=2.04e-03
Iter  400: loss=2.892225e-03 | grad=1.3942e+00 | lr=2.04e-03
✓ Saved loss history for timestep 9 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_9.json

 Final loss: 2.416346e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=8/50 (t=0.160)
======================================================================

============================================================
Training timestep n=8 (BACKWARD STEP)
Time: t=0.160, Future models: 42
============================================================
Iter    0: loss=6.044532e-03 | grad=1.5603e+00 | lr=8.10e-03
Iter    1: loss=1.107606e-01 | grad=2.9292e+00 | lr=8.10e-03
Iter    2: loss=1.178160e-01 | grad=1.5972e+00 | lr=8.10e-03
Iter    3: loss=8.642209e-02 | grad=3.0443e+00 | lr=8.10e-03
Iter    4: loss=4.724970e-02 | grad=9.0507e-01 | lr=8.10e-03
Iter    5: loss=8.242963e-02 | grad=3.9521e+00 | lr=8.10e-03
Iter    6: loss=8.290625e-02 | grad=1.6491e+00 | lr=8.10e-03
Iter    7: loss=2.339919e-02 | grad=1.0987e+00 | lr=8.10e-03
Iter    8: loss=6.038581e-02 | grad=1.9886e+00 | lr=8.10e-03
Iter    9: loss=3.739117e-02 | grad=1.7037e+00 | lr=8.10e-03
Iter  100: loss=4.667083e-03 | grad=1.4421e+00 | lr=4.05e-03
Iter  200: loss=3.256652e-03 | grad=9.0358e-01 | lr=4.05e-03
Iter  300: loss=2.557124e-03 | grad=2.5366e-01 | lr=2.03e-03
Iter  400: loss=2.669041e-03 | grad=8.6820e-01 | lr=2.03e-03
✓ Saved loss history for timestep 8 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_8.json

 Final loss: 2.506488e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=7/50 (t=0.140)
======================================================================

============================================================
Training timestep n=7 (BACKWARD STEP)
Time: t=0.140, Future models: 43
============================================================
Iter    0: loss=6.659941e-03 | grad=2.3053e+00 | lr=8.06e-03
Iter    1: loss=1.013622e-01 | grad=3.3344e+00 | lr=8.06e-03
Iter    2: loss=1.631015e-01 | grad=1.8544e+00 | lr=8.06e-03
Iter    3: loss=9.804310e-02 | grad=2.4706e+00 | lr=8.06e-03
Iter    4: loss=5.490210e-02 | grad=1.1414e+00 | lr=8.06e-03
Iter    5: loss=9.431729e-02 | grad=4.3242e+00 | lr=8.06e-03
Iter    6: loss=8.902098e-02 | grad=1.4222e+00 | lr=8.06e-03
Iter    7: loss=7.174817e-02 | grad=2.5352e+00 | lr=8.06e-03
Iter    8: loss=6.035961e-02 | grad=2.8469e+00 | lr=8.06e-03
Iter    9: loss=5.544615e-02 | grad=1.5357e+00 | lr=8.06e-03
Iter  100: loss=1.326794e-02 | grad=3.5190e+00 | lr=4.03e-03
Iter  200: loss=3.315082e-03 | grad=5.7394e-01 | lr=2.02e-03
Iter  300: loss=3.826910e-03 | grad=2.4249e+00 | lr=2.02e-03
Iter  400: loss=2.441759e-03 | grad=2.5825e-01 | lr=1.01e-03
✓ Saved loss history for timestep 7 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_7.json

 Final loss: 2.450536e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=6/50 (t=0.120)
======================================================================

============================================================
Training timestep n=6 (BACKWARD STEP)
Time: t=0.120, Future models: 44
============================================================
Iter    0: loss=6.583491e-03 | grad=1.9968e+00 | lr=8.02e-03
Iter    1: loss=1.878551e-01 | grad=3.9469e+00 | lr=8.02e-03
Iter    2: loss=7.423436e-02 | grad=3.6061e+00 | lr=8.02e-03
Iter    3: loss=1.529676e-01 | grad=2.1072e+00 | lr=8.02e-03
Iter    4: loss=1.786630e-01 | grad=3.3749e+00 | lr=8.02e-03
Iter    5: loss=1.076059e-01 | grad=4.1796e+00 | lr=8.02e-03
Iter    6: loss=9.385946e-02 | grad=1.4425e+00 | lr=8.02e-03
Iter    7: loss=1.254401e-01 | grad=4.5471e+00 | lr=8.02e-03
Iter    8: loss=1.298853e-01 | grad=3.9099e+00 | lr=8.02e-03
Iter    9: loss=9.264880e-02 | grad=2.5348e+00 | lr=8.02e-03
Iter  100: loss=4.867679e-03 | grad=4.6740e-01 | lr=4.01e-03
Iter  200: loss=3.443770e-03 | grad=2.2464e-01 | lr=2.01e-03
Iter  300: loss=2.788148e-03 | grad=5.1591e-01 | lr=2.01e-03
Iter  400: loss=2.533108e-03 | grad=8.2603e-01 | lr=1.00e-03
✓ Saved loss history for timestep 6 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_6.json

 Final loss: 2.492190e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=5/50 (t=0.100)
======================================================================

============================================================
Training timestep n=5 (BACKWARD STEP)
Time: t=0.100, Future models: 45
============================================================
Iter    0: loss=5.495947e-03 | grad=1.8014e+00 | lr=7.98e-03
Iter    1: loss=1.700652e-01 | grad=4.0816e+00 | lr=7.98e-03
Iter    2: loss=6.008969e-02 | grad=2.0927e+00 | lr=7.98e-03
Iter    3: loss=1.583614e-01 | grad=2.9500e+00 | lr=7.98e-03
Iter    4: loss=1.384299e-01 | grad=4.3400e+00 | lr=7.98e-03
Iter    5: loss=1.041127e-01 | grad=1.9928e+00 | lr=7.98e-03
Iter    6: loss=5.778185e-02 | grad=3.4580e+00 | lr=7.98e-03
Iter    7: loss=7.110047e-02 | grad=3.5663e+00 | lr=7.98e-03
Iter    8: loss=5.102143e-02 | grad=1.8212e+00 | lr=7.98e-03
Iter    9: loss=4.294762e-02 | grad=1.6763e+00 | lr=7.98e-03
Iter  100: loss=4.904776e-03 | grad=8.3475e-01 | lr=3.99e-03
Iter  200: loss=8.160741e-03 | grad=1.3957e+00 | lr=3.99e-03
Iter  300: loss=4.001668e-03 | grad=2.0286e-01 | lr=2.00e-03
Iter  400: loss=2.763275e-03 | grad=7.0878e-01 | lr=2.00e-03
✓ Saved loss history for timestep 5 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_5.json

 Final loss: 2.568880e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=4/50 (t=0.080)
======================================================================

============================================================
Training timestep n=4 (BACKWARD STEP)
Time: t=0.080, Future models: 46
============================================================
Iter    0: loss=7.381736e-03 | grad=1.3870e+00 | lr=7.94e-03
Iter    1: loss=1.118955e-01 | grad=3.6448e+00 | lr=7.94e-03
Iter    2: loss=1.407059e-01 | grad=1.6036e+00 | lr=7.94e-03
Iter    3: loss=1.600107e-01 | grad=2.3938e+00 | lr=7.94e-03
Iter    4: loss=7.229987e-02 | grad=2.3615e+00 | lr=7.94e-03
Iter    5: loss=6.504794e-02 | grad=2.2776e+00 | lr=7.94e-03
Iter    6: loss=7.678147e-02 | grad=2.8609e+00 | lr=7.94e-03
Iter    7: loss=4.542552e-02 | grad=1.3239e+00 | lr=7.94e-03
Iter    8: loss=5.025043e-02 | grad=2.5723e+00 | lr=7.94e-03
Iter    9: loss=5.887555e-02 | grad=2.0283e+00 | lr=7.94e-03
Iter  100: loss=5.968762e-03 | grad=1.0508e+00 | lr=7.94e-03
Iter  200: loss=5.350400e-03 | grad=1.5300e+00 | lr=7.94e-03
Iter  300: loss=3.384339e-03 | grad=9.4981e-01 | lr=3.97e-03
Iter  400: loss=3.219459e-03 | grad=1.4374e+00 | lr=3.97e-03
✓ Saved loss history for timestep 4 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_4.json

 Final loss: 2.492222e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=3/50 (t=0.060)
======================================================================

============================================================
Training timestep n=3 (BACKWARD STEP)
Time: t=0.060, Future models: 47
============================================================
Iter    0: loss=5.644031e-03 | grad=1.3324e+00 | lr=7.90e-03
Iter    1: loss=8.925531e-02 | grad=3.1632e+00 | lr=7.90e-03
Iter    2: loss=1.191417e-01 | grad=1.8170e+00 | lr=7.90e-03
Iter    3: loss=9.309544e-02 | grad=2.1020e+00 | lr=7.90e-03
Iter    4: loss=3.193720e-02 | grad=1.0092e+00 | lr=7.90e-03
Iter    5: loss=1.062639e-01 | grad=1.7743e+00 | lr=7.90e-03
Iter    6: loss=5.336905e-02 | grad=1.9998e+00 | lr=7.90e-03
Iter    7: loss=6.891166e-02 | grad=1.7715e+00 | lr=7.90e-03
Iter    8: loss=7.231401e-02 | grad=2.1041e+00 | lr=7.90e-03
Iter    9: loss=2.515067e-02 | grad=1.2126e+00 | lr=7.90e-03
Iter  100: loss=4.323626e-03 | grad=4.0671e-01 | lr=3.95e-03
Iter  200: loss=4.389678e-03 | grad=1.2730e+00 | lr=3.95e-03
Iter  300: loss=3.386055e-03 | grad=3.8608e-01 | lr=3.95e-03
Iter  400: loss=3.332548e-03 | grad=1.5466e+00 | lr=3.95e-03
✓ Saved loss history for timestep 3 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_3.json

 Final loss: 2.421539e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=2/50 (t=0.040)
======================================================================

============================================================
Training timestep n=2 (BACKWARD STEP)
Time: t=0.040, Future models: 48
============================================================
Iter    0: loss=5.202706e-03 | grad=1.3122e+00 | lr=7.86e-03
Iter    1: loss=7.254350e-02 | grad=3.2422e+00 | lr=7.86e-03
Iter    2: loss=1.487716e-01 | grad=1.6333e+00 | lr=7.86e-03
Iter    3: loss=8.749599e-02 | grad=2.9469e+00 | lr=7.86e-03
Iter    4: loss=3.236089e-02 | grad=1.5804e+00 | lr=7.86e-03
Iter    5: loss=8.839213e-02 | grad=3.7186e+00 | lr=7.86e-03
Iter    6: loss=7.995874e-02 | grad=2.6058e+00 | lr=7.86e-03
Iter    7: loss=3.155405e-02 | grad=1.6068e+00 | lr=7.86e-03
Iter    8: loss=5.071355e-02 | grad=2.5709e+00 | lr=7.86e-03
Iter    9: loss=6.040608e-02 | grad=2.7324e+00 | lr=7.86e-03
Iter  100: loss=3.758680e-03 | grad=6.3895e-01 | lr=3.93e-03
Iter  200: loss=3.384917e-03 | grad=6.7533e-01 | lr=3.93e-03
Iter  300: loss=2.543694e-03 | grad=1.3543e-01 | lr=1.97e-03
Iter  400: loss=2.523322e-03 | grad=5.3147e-01 | lr=1.97e-03
✓ Saved loss history for timestep 2 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_2.json

 Final loss: 2.404287e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=1/50 (t=0.020)
======================================================================

============================================================
Training timestep n=1 (BACKWARD STEP)
Time: t=0.020, Future models: 49
============================================================
Iter    0: loss=5.661065e-03 | grad=1.4383e+00 | lr=7.82e-03
Iter    1: loss=9.719375e-02 | grad=2.8303e+00 | lr=7.82e-03
Iter    2: loss=9.644316e-02 | grad=1.4922e+00 | lr=7.82e-03
Iter    3: loss=9.118599e-02 | grad=1.9289e+00 | lr=7.82e-03
Iter    4: loss=3.388349e-02 | grad=1.4483e+00 | lr=7.82e-03
Iter    5: loss=1.125639e-01 | grad=1.8425e+00 | lr=7.82e-03
Iter    6: loss=8.421266e-02 | grad=3.3311e+00 | lr=7.82e-03
Iter    7: loss=3.445580e-02 | grad=1.9276e+00 | lr=7.82e-03
Iter    8: loss=6.546810e-02 | grad=2.2252e+00 | lr=7.82e-03
Iter    9: loss=6.724931e-02 | grad=3.2788e+00 | lr=7.82e-03
Iter  100: loss=9.126094e-03 | grad=2.2761e+00 | lr=7.82e-03
Iter  200: loss=2.980171e-03 | grad=1.0189e+00 | lr=3.91e-03
Iter  300: loss=2.481967e-03 | grad=7.0976e-01 | lr=1.96e-03
Iter  400: loss=2.261738e-03 | grad=2.2356e-01 | lr=1.96e-03
✓ Saved loss history for timestep 1 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_1.json

 Final loss: 2.213710e-03
Iterations used: 500

======================================================================
TRAINING TIMESTEP n=0/50 (t=0.000)
======================================================================

============================================================
Training timestep n=0 (BACKWARD STEP)
Time: t=0.000, Future models: 50
============================================================
Iter    0: loss=5.619408e-03 | grad=1.2124e+00 | lr=7.78e-03
Iter    1: loss=8.795621e-02 | grad=3.0032e+00 | lr=7.78e-03
Iter    2: loss=9.309688e-02 | grad=1.6818e+00 | lr=7.78e-03
Iter    3: loss=1.276218e-01 | grad=1.4496e+00 | lr=7.78e-03
Iter    4: loss=3.141304e-02 | grad=1.4761e+00 | lr=7.78e-03
Iter    5: loss=1.746516e-01 | grad=1.7818e+00 | lr=7.78e-03
Iter    6: loss=1.058211e-01 | grad=1.9239e+00 | lr=7.78e-03
Iter    7: loss=5.565914e-02 | grad=2.1860e+00 | lr=7.78e-03
Iter    8: loss=7.885689e-02 | grad=1.5540e+00 | lr=7.78e-03
Iter    9: loss=5.302622e-02 | grad=2.6143e+00 | lr=7.78e-03
Iter  100: loss=4.006389e-03 | grad=2.8722e-01 | lr=3.89e-03
Iter  200: loss=3.662633e-03 | grad=4.5767e-01 | lr=3.89e-03
Iter  300: loss=2.915918e-03 | grad=5.3637e-01 | lr=3.89e-03
Iter  400: loss=2.622676e-03 | grad=4.1520e-01 | lr=1.95e-03
✓ Saved loss history for timestep 0 to /cfs/klemming/projects/supr/naiss2024-22-1707/DeepBSVIE/reflected_2025-11-15_17-41-09/models/loss_history_timestep_0.json

 Final loss: 2.412282e-03
Iterations used: 500
Elapsed time: 8.9346 minutes

======================================================================
VALIDATING AGAINST ANALYTICAL SOLUTION
======================================================================
Traceback (most recent call last):
  File "/cfs/klemming/home/p/pucci/PycharmProjects/DeepBSVIE/main.py", line 151, in <module>
    validation_metrics = validate_against_analytical(
  File "/cfs/klemming/home/p/pucci/PycharmProjects/DeepBSVIE/Evaluation.py", line 209, in validate_against_analytical
    Y_analytical = result.analytical_Y(times, x_np)  # Shape: [batch_size, 1, N+1]
  File "/cfs/klemming/home/p/pucci/PycharmProjects/DeepBSVIE/Evaluation.py", line 127, in analytical_Y
    raise ValueError(f"Unknown example_type: {self.example_type}")
ValueError: Unknown example_type: reflected
